import time
import os
import requests
import psutil
from tokenizers import Tokenizer, models, trainers, pre_tokenizers
from tokenizers.processors import TemplateProcessing
import cProfile
import pstats
import io
import threading
from memory_profiler import memory_usage

# ======================
# é…ç½®å‚æ•°
# ======================
DATA_URL = "https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt"
OUTPUT_TOKENIZER_PATH = "./tinystories_bpe_5k.json"
VOCAB_SIZE = 5000
SPECIAL_TOKENS = ["<|endoftext|>"]
ENABLE_PROFILING = True

# ======================
# ä¸‹è½½æ•°æ®å‡½æ•°
# ======================
def download_file(url, save_path="tinystories_valid.txt"):
    print(f"ğŸ“¥ æ­£åœ¨ä» {url} ä¸‹è½½æ•°æ®...")
    response = requests.get(url)
    if response.status_code == 200:
        with open(save_path, "wb") as f:
            f.write(response.content)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {save_path}")
        return save_path
    else:
        raise RuntimeError(f"âŒ ä¸‹è½½å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")

# ======================
# è¯»å–æ–‡æœ¬æ•°æ®
# ======================
def load_text(filepath):
    print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡æœ¬æ–‡ä»¶: {filepath}")
    with open(filepath, "r", encoding="utf-8") as f:
        text = f.read()
    print(f"ğŸ“ æ–‡æœ¬æ€»é•¿åº¦: ~{len(text) // 1000}k å­—ç¬¦")
    return text

# ======================
# è®­ç»ƒ BPE åˆ†è¯å™¨æ ¸å¿ƒå‡½æ•°
# ======================
def train_bpe_core(text: str, vocab_size: int, special_tokens: list, output_path: str):
    """BPEè®­ç»ƒçš„æ ¸å¿ƒé€»è¾‘ï¼Œä¸åŒ…å«æ€§èƒ½åˆ†æ"""
    print("\nğŸ”§ å¼€å§‹åˆå§‹åŒ– BPE Tokenizer...")
    start_time = time.time()

    # 1. åˆ›å»º tokenizer
    tokenizer = Tokenizer(models.BPE())

    # 2. è®¾ç½® Pre-tokenizer
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

    # 3. è®¾ç½® BPE Trainer
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        show_progress=True,
        min_frequency=2
    )

    # 4. å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ BPE æ¨¡å‹...")
    tokenizer.train_from_iterator([text], trainer)

    # 5. è®¾ç½®åå¤„ç†
    tokenizer.post_processor = TemplateProcessing(
        single="<|endoftext|> $A <|endoftext|>",
        pair="<|endoftext|> $A <|endoftext|> $B <|endoftext|>",
        special_tokens=[("<|endoftext|>", 1)],
    )

    # 6. ä¿å­˜ tokenizer
    tokenizer.save(output_path)
    print(f"ğŸ’¾ BPE åˆ†è¯å™¨å·²ä¿å­˜è‡³: {output_path}")

    # 7. åˆ†æè¯æ±‡è¡¨
    vocab = tokenizer.get_vocab()
    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])
    max_len_token = max([len(token) for token, _ in sorted_vocab], default=0)
    longest_tokens = [token for token, _ in sorted_vocab if len(token) == max_len_token]

    elapsed_time = time.time() - start_time
    print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  è®­ç»ƒè€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"ğŸ”¤ è¯æ±‡è¡¨å¤§å°: {len(vocab)} (ç›®æ ‡: {vocab_size})")
    print(f"ğŸ“ è¯æ±‡è¡¨ä¸­æœ€é•¿ token çš„é•¿åº¦: {max_len_token}")
    if longest_tokens:
        print(f"   ç¤ºä¾‹æœ€é•¿ token(s): {longest_tokens[:5]}")

    return tokenizer, elapsed_time, max_len_token, longest_tokens

# ======================
# æ€§èƒ½åˆ†æåŒ…è£…å™¨
# ======================
def run_with_profiling(text, vocab_size, special_tokens, output_path):
    """è¿è¡Œè®­ç»ƒå¹¶åŒæ—¶è¿›è¡Œæ€§èƒ½åˆ†æ"""
    print("\nğŸ” [æ€§èƒ½åˆ†æ] å¼€å§‹å†…å­˜å’ŒCPUåˆ†æ...")
    
    # å†…å­˜åˆ†æ
    mem_before = psutil.virtual_memory().used / (1024 * 1024)  # MB
    
    # CPUåˆ†æ
    pr = cProfile.Profile()
    pr.enable()
    
    # æ‰§è¡Œè®­ç»ƒï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    result = train_bpe_core(text, vocab_size, special_tokens, output_path)
    
    pr.disable()
    
    # å†…å­˜ä½¿ç”¨è®¡ç®—
    mem_after = psutil.virtual_memory().used / (1024 * 1024)  # MB
    peak_mem_usage = mem_after - mem_before
    
    # è¾“å‡ºCPUåˆ†æç»“æœ
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats(10)
    
    print(f"ğŸ” [å†…å­˜] å³°å€¼å†…å­˜ä½¿ç”¨: {peak_mem_usage:.2f} MiB")
    print("âš¡ [CPUåˆ†æ] æœ€è€—æ—¶çš„å‡½æ•°:")
    print(s.getvalue())
    
    return result, peak_mem_usage

# ======================
# ä¸»è®­ç»ƒå‡½æ•°
# ======================
def train_bpe_tokenizer(text: str, vocab_size: int, special_tokens: list, output_path: str):
    """ä¸»è®­ç»ƒå‡½æ•°ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¿›è¡Œæ€§èƒ½åˆ†æ"""
    if ENABLE_PROFILING:
        result, peak_mem = run_with_profiling(text, vocab_size, special_tokens, output_path)
        tokenizer, elapsed_time, max_token_len, longest_tokens = result
        return tokenizer, elapsed_time, max_token_len, longest_tokens, peak_mem
    else:
        tokenizer, elapsed_time, max_token_len, longest_tokens = train_bpe_core(
            text, vocab_size, special_tokens, output_path
        )
        return tokenizer, elapsed_time, max_token_len, longest_tokens, 0

# ======================
# æµ‹è¯•åˆ†è¯å™¨
# ======================
def test_tokenizer(tokenizer_path, test_texts):
    """æµ‹è¯•è®­ç»ƒå¥½çš„åˆ†è¯å™¨"""
    print("\nğŸ§ª æµ‹è¯•åˆ†è¯å™¨...")
    tokenizer = Tokenizer.from_file(tokenizer_path)
    
    for i, text in enumerate(test_texts):
        print(f"\næµ‹è¯•æ–‡æœ¬ {i+1}: '{text}'")
        encoding = tokenizer.encode(text)
        print(f"Tokens: {encoding.tokens}")
        print(f"Token IDs: {encoding.ids}")
        print(f"Tokenæ•°é‡: {len(encoding.tokens)}")

# ======================
# ä¸»æ‰§è¡Œæµç¨‹
# ======================
def main():
    print("=" * 60)
    print("ğŸ¤– TinyStories BPE åˆ†è¯å™¨è®­ç»ƒè„šæœ¬")
    print("=" * 60)
    
    # --- 1. ä¸‹è½½æ•°æ® ---
    try:
        data_path = download_file(DATA_URL)
    except Exception as e:
        print(f"{e}ï¼Œå°†å°è¯•ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ tinystories_valid.txtï¼ˆå¦‚æœ‰ï¼‰")
        data_path = "tinystories_valid.txt"
        if not os.path.exists(data_path):
            try:
                backup_url = "https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt"
                print(f"ğŸ”„ å°è¯•å¤‡ç”¨URL: {backup_url}")
                data_path = download_file(backup_url, "tinystories_valid.txt")
            except:
                raise RuntimeError(f"è¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®å¹¶ä¿å­˜ä¸º {data_path}")

    # --- 2. åŠ è½½å…¨éƒ¨æ–‡æœ¬å†…å®¹ ---
    text = load_text(data_path)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    lines = text.split('\n')
    words = text.split()
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {len(lines)} è¡Œ, {len(words)} ä¸ªå•è¯")

    # --- 3. è®­ç»ƒ BPE åˆ†è¯å™¨ ---
    print("\n" + "="*50)
    print(f"ğŸ—ï¸  å¼€å§‹è®­ç»ƒ BPE åˆ†è¯å™¨ (Vocab Size = {VOCAB_SIZE})")
    print("="*50)

    # åªè°ƒç”¨ä¸€æ¬¡è®­ç»ƒå‡½æ•°
    tokenizer, train_time, max_token_len, longest_toks, peak_mem = train_bpe_tokenizer(
        text=text,
        vocab_size=VOCAB_SIZE,
        special_tokens=SPECIAL_TOKENS,
        output_path=OUTPUT_TOKENIZER_PATH
    )

    # --- 4. è¾“å‡ºç³»ç»Ÿä¿¡æ¯ ---
    mem_info = psutil.virtual_memory()
    print("\nğŸ–¥ï¸  ç³»ç»Ÿå†…å­˜ä¿¡æ¯:")
    print(f"   æ€»å†…å­˜: {mem_info.total / (1024**3):.2f} GiB")
    print(f"   å¯ç”¨å†…å­˜: {mem_info.available / (1024**3):.2f} GiB")
    print(f"   å†…å­˜ä½¿ç”¨ç‡: {mem_info.percent}%")

    # --- 5. æµ‹è¯•åˆ†è¯å™¨ ---
    test_texts = [
        "Once upon a time,",
        "The little girl was happy.",
        "They lived in a beautiful house near the forest."
    ]
    test_tokenizer(OUTPUT_TOKENIZER_PATH, test_texts)

    # --- 6. æœ€ç»ˆæŠ¥å‘Š ---
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("="*60)
    print(f"ğŸ”§ åˆ†è¯å™¨æ–‡ä»¶: {OUTPUT_TOKENIZER_PATH}")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {train_time:.2f} ç§’ (~{train_time/60:.2f} åˆ†é’Ÿ)")
    print(f"ğŸ”¤ è¯æ±‡è¡¨å¤§å°: {VOCAB_SIZE}")
    print(f"ğŸ“ è¯æ±‡è¡¨ä¸­æœ€é•¿ Token é•¿åº¦: {max_token_len}")
    if longest_toks:
        print(f"   æœ€é•¿ Token ç¤ºä¾‹: {longest_toks[:3]}")
    
    if ENABLE_PROFILING:
        print(f"ğŸ” å³°å€¼å†…å­˜ä½¿ç”¨: {peak_mem:.2f} MiB")
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
    if os.path.exists(OUTPUT_TOKENIZER_PATH):
        file_size = os.path.getsize(OUTPUT_TOKENIZER_PATH) / 1024
        print(f"ğŸ’¾ åˆ†è¯å™¨æ–‡ä»¶å¤§å°: {file_size:.2f} KB")

if __name__ == "__main__":
    main()