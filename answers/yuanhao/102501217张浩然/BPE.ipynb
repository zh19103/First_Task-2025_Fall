{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfac640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import sys\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Tuple, Iterable, Iterator\n",
    "\n",
    "import psutil\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# ======================\n",
    "# Configuration Parameters\n",
    "# ======================\n",
    "CONFIG = {\n",
    "    \"data_url\": \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt\",\n",
    "    \"output_path\": \"./tinystories_bpe_5k.json\",\n",
    "    \"vocab_size\": 5000,\n",
    "    \"special_tokens\": [\"<|endoftext|>\"],\n",
    "    \"enable_profiling\": True\n",
    "}\n",
    "\n",
    "# ======================\n",
    "# Data Handling Utilities\n",
    "# ======================\n",
    "def download_dataset(url: str, filename: str = \"dataset.txt\") -> str:\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Dataset {filename} already exists.\")\n",
    "        return filename\n",
    "    print(f\"Downloading dataset from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"Dataset saved to {filename}\")\n",
    "    return filename\n",
    "\n",
    "def load_text_data(filepath: str) -> str:\n",
    "    \"\"\"Load and preprocess text data\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Clean whitespace and non-ASCII characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
    "    print(f\"ðŸ“– Loaded {len(text)} characters\")\n",
    "    return text\n",
    "\n",
    "# ======================\n",
    "# BPE Tokenizer Implementation\n",
    "# ======================\n",
    "class BPETokenizer(Tokenizer):\n",
    "    def __init__(self, vocab: Dict[str, int], merges: Dict[Tuple[str, str], int], special_tokens: Optional[Dict[str, int]] = None):\n",
    "        super().__init__(models.BPE.from_file(\"tokenizer.json\"))\n",
    "        \n",
    "        # Initialize special tokens\n",
    "        self.special_tokens = {\"<|endoftext|>\": 50256} if not specials else specials\n",
    "        self._normalize_special_tokens()\n",
    "        \n",
    "        # Build vocabulary mappings\n",
    "        self.vocab = vocab.copy()\n",
    "        self.merges = {tuple(k): v for k, v in merges.items()}\n",
    "        self._build_mappings()\n",
    "\n",
    "    def _normalize_special_tokens(self):\n",
    "        \"\"\"Normalize special token formatting\"\"\"\n",
    "        self.special_tokens = {k.lower(): v for k, v in self.special_tokens.items()}\n",
    "        self.special_tokens.setdefault(\"<|endoftext|>\", 50256)\n",
    "\n",
    "    def _build_mappings(self):\n",
    "        \"\"\"Construct bidirectional vocabulary mappings\"\"\"\n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        self.token_to_id = {k: v for k, v in self.vocab.items()}\n",
    "        self.token_to_id.update(self.special_tokens)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, vocab_path: str, merges_path: Optional[str] = None):\n",
    "        \"\"\"Load pretrained tokenizer\"\"\"\n",
    "        tokenizer = cls(vocab={}, merges={}, special_tokens=SPECIAL_TOKENS)\n",
    "        \n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            tokenizer.vocab = json.load(f)\n",
    "        \n",
    "        # Load merge operations\n",
    "        if merges_path:\n",
    "            with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                merges = json.load(f)\n",
    "                tokenizer.merges = {tuple(k): v for k, v in merges.items()}\n",
    "        \n",
    "        tokenizer._build_mappings()\n",
    "        return tokenizer\n",
    "\n",
    "# ======================\n",
    "# Training Pipeline\n",
    "# ======================\n",
    "def train_tokenizer():\n",
    "    data_path = download_dataset(CONFIG[\"data_url\"])\n",
    "    \n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<|unk|>\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=CONFIG[\"vocab_size\"],\n",
    "        special_tokens=[\"<|unk|>\", \"<|endoftext|>\"],\n",
    "        min_frequency=2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining BPE model ({CONFIG['vocab_size']} vocabulary size)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokenizer.train(files=[data_path], trainer=trainer)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[SOS] $A [EOS]\",\n",
    "        pair=\"[SEP] $A [SEP] $B [SEP]\",\n",
    "        special_tokens=[(\"[SOS]\", 1), (\"[EOS]\", 2), (\"[SEP]\", 3)]\n",
    "    )\n",
    "    \n",
    "    if CONFIG[\"enable_profiling\"]:\n",
    "        analyze_performance(elapsed, tokenizer)\n",
    "\n",
    "    tokenizer.save(CONFIG[\"output_path\"])\n",
    "    print(f\"Tokenizer saved to {CONFIG['output_path']}\")\n",
    "    return tokenizer\n",
    "\n",
    "# ======================\n",
    "# Performance Analysis\n",
    "# ======================\n",
    "def analyze_performance(elapsed_time: float, tokenizer: Tokenizer):\n",
    "    print(\"\\nPerformance Analysis Report:\")\n",
    "    print(f\"Total training time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Memory usage: {mem.percent}%\")\n",
    "    \n",
    "    vocab = tokenizer.get_vocab()\n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "    print(f\"Vocabulary stats:\")\n",
    "    print(f\"  Total entries: {len(vocab)}\")\n",
    "    print(f\"  Max token length: {max(len(k) for k in sorted_vocab)}\")\n",
    "\n",
    "# ======================\n",
    "# Testing & Validation\n",
    "# ======================\n",
    "def test_tokenizer(tokenizer):\n",
    "    test_cases = [\n",
    "        \"Once upon a time,\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"AI models are transforming the world.\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_cases:\n",
    "        encoded = tokenizer.encode(text)\n",
    "        print(f\"\\nInput: {text}\")\n",
    "        print(f\"Token IDs: {encoded.ids}\")\n",
    "        print(f\"Tokens: {encoded.tokens}\")\n",
    "        print(f\"Decoded: {tokenizer.decode(encoded.ids)}\")\n",
    "\n",
    "# ======================\n",
    "# Main Execution Flow\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        tokenizer = train_tokenizer()\n",
    "        test_tokenizer(tokenizer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
