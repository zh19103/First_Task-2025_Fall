import json
import re
from typing import Iterable, Iterator, List, Optional, Dict, Tuple

try:
    # Prefer the third-party 'regex' module for GPT-2 parity
    import regex as regex_mod  # type: ignore
except Exception:
    regex_mod = None  # fallback to 're'


# Canonical GPT-2/tiktoken pretokenizer pattern.
# Prefer 'regex' version; fallback to a close 're' approximation.
if regex_mod:
    _GPT2_PAT = regex_mod.compile(
        r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""",
        flags=regex_mod.IGNORECASE,
    )
else:
    # Best-effort fallback without Unicode properties
    _GPT2_PAT = re.compile(
        r"""'(?:s|t|re|ve|m|ll|d)| ?[A-Za-z]+| ?\d+| ?[^\sA-Za-z0-9]+|\s+(?!\S)|\s+""",
        re.IGNORECASE,
    )


class Tokenizer:
    def __init__(self, vocab: Dict[str, int], merges: Dict[tuple, int], special_tokens: Optional[Dict[str, int]] = None):
        """
        构造：补全0..255字节；使用GPT-2/tiktoken式BPE；特殊token直通
        """
        # 1) 基础
        self.vocab: Dict[str, int] = vocab.copy()
        self.merges: Dict[Tuple[str, str], int] = {tuple(k): v for k, v in (merges or {}).items()}
        # Normalize specials; enforce eot id per tiktoken
        self.special_tokens: Dict[str, int] = (special_tokens or {}).copy()
        if "<|endoftext|>" in self.special_tokens:
            self.special_tokens["<|endoftext|>"] = 50256  # enforce canonical id

        # 2) 确保0..255字节存在（latin-1 单字符）
        for byte_val in range(256):
            ch = bytes([byte_val]).decode("latin-1")
            if ch not in self.vocab:
                self.vocab[ch] = byte_val

        # 3) 反向映射
        self.id_to_token: Dict[int, str] = {v: k for k, v in self.vocab.items()}
        for tok, tid in self.special_tokens.items():
            self.id_to_token[tid] = tok

        # 4) 特殊token正则（最长优先）
        self._special_sorted: List[str] = sorted(self.special_tokens.keys(), key=len, reverse=True)
        self._special_re: Optional[re.Pattern] = None
        if self._special_sorted:
            pat = "(" + "|".join(re.escape(s) for s in self._special_sorted) + ")"
            # Use the same engine as Python 're' for splitting
            self._special_re = re.compile(pat)

        # 5) 预分词器
        self._pretok_re = _GPT2_PAT

    @classmethod
    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: Optional[Dict[str, int]] = None):
        # 词表
        with open(vocab_filepath, "r", encoding="utf-8") as f:
            if vocab_filepath.endswith(".json"):
                vocab = json.load(f)
            else:
                vocab = {}
                for ln, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    parts = line.split("\t")
                    if len(parts) != 2:
                        raise ValueError(f"vocab 第 {ln} 行格式错误，应为 token<TAB>id")
                    token, tid = parts
                    vocab[token] = int(tid)

        # merges -> rank
        merges: Dict[Tuple[str, str], int] = {}
        if merges_filepath.endswith(".txt"):
            with open(merges_filepath, "r", encoding="utf-8") as f:
                rank = 0
                for raw in f:
                    line = raw.strip()
                    if not line or line.startswith("#"):
                        continue
                    parts = line.split()
                    if len(parts) != 2:
                        continue
                    a, b = parts
                    merges[(a, b)] = rank
                    rank += 1
        else:
            with open(merges_filepath, "r", encoding="utf-8") as f:
                merges_raw = json.load(f)
            for k, v in merges_raw.items():
                if isinstance(k, str):
                    parts = k.split()
                    if len(parts) != 2:
                        continue
                    a, b = parts
                else:
                    if len(k) != 2:
                        continue
                    a, b = k
                merges[(a, b)] = int(v)

        # Normalize specials here as well; __init__ will re-enforce
        specials = (special_tokens or {}).copy()
        if "<|endoftext|>" in specials:
            specials["<|endoftext|>"] = 50256

        return cls(vocab, merges, specials)

    def _bytes_to_tokens(self, bs: bytes) -> List[str]:
        return [bytes([b]).decode("latin-1") for b in bs]

    def _select_best_pair(self, tokens: List[str]) -> Tuple[int, int]:
        INF = 10**18
        best_rank = INF
        best_idx = -1
        for i in range(len(tokens) - 1):
            pair = (tokens[i], tokens[i + 1])
            rank = self.merges.get(pair)
            if rank is None:
                continue
            if rank < best_rank:
                best_rank = rank
                best_idx = i
                if best_rank == 0:
                    break
        return best_rank, best_idx

    def _merge_round(self, tokens: List[str]) -> Optional[List[str]]:
        INF = 10**18
        best_rank, first_idx = self._select_best_pair(tokens)
        if best_rank == INF:
            return None
        target = (tokens[first_idx], tokens[first_idx + 1])
        out: List[str] = []
        i = 0
        n = len(tokens)
        while i < n:
            if i < n - 1 and (tokens[i], tokens[i + 1]) == target:
                out.append(tokens[i] + tokens[i + 1])
                i += 2
            else:
                out.append(tokens[i])
                i += 1
        return out

    def _bpe_encode_bytes_block(self, bs: bytes) -> List[str]:
        tokens = self._bytes_to_tokens(bs)
        if len(tokens) <= 1:
            return tokens
        while True:
            new_tokens = self._merge_round(tokens)
            if new_tokens is None:
                break
            tokens = new_tokens
        return tokens

    def _encode_chunk(self, text: str) -> List[int]:
        out_ids: List[int] = []
        # Use the same engine for findall as compiled _GPT2_PAT (regex or re)
        if regex_mod and isinstance(self._pretok_re, regex_mod.Pattern):
            pieces = self._pretok_re.findall(text)
        else:
            pieces = _GPT2_PAT.findall(text)
        for piece in pieces:
            if not piece:
                continue
            bs = piece.encode("utf-8")
            token_strs = self._bpe_encode_bytes_block(bs)
            for t in token_strs:
                tid = self.vocab.get(t)
                if tid is not None:
                    out_ids.append(tid)
                else:
                    for ch in t:
                        b = ord(ch)
                        ch_str = bytes([b]).decode("latin-1")
                        tid_b = self.vocab.get(ch_str)
                        if tid_b is None:
                            raise KeyError(f"缺少基础字节 token: {repr(ch_str)}")
                        out_ids.append(tid_b)
        return out_ids

    def encode(self, text: str) -> List[int]:
        if not text:
            return []

        # Split raw text by special tokens first (longest-first)
        if self._special_re:
            ids: List[int] = []
            pos = 0
            for m in self._special_re.finditer(text):
                s, e = m.span()
                if s > pos:
                    ids.extend(self._encode_chunk(text[pos:s]))
                tok = m.group(0)
                stid = self.special_tokens.get(tok)
                if stid is None:
                    ids.extend(self._encode_chunk(tok))
                else:
                    ids.append(stid)
                pos = e
            if pos < len(text):
                ids.extend(self._encode_chunk(text[pos:]))
            return ids

        return self._encode_chunk(text)

    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:
        for text in iterable:
            yield from self.encode(text)

    def decode(self, ids: List[int]) -> str:
        if not ids:
            return ""
        toks: List[str] = []
        for tid in ids:
            if tid not in self.id_to_token:
                raise ValueError(f"未知token ID: {tid}")
            toks.append(self.id_to_token[tid])
        s = "".join(toks)
        try:
            return s.encode("latin-1").decode("utf-8", errors="replace")
        except UnicodeDecodeError:
            return s